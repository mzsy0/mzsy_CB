"""
é‚¢ä¸è¡Œï½œç­–ç•¥åˆ†äº«ä¼š
è‚¡ç¥¨é‡åŒ–ç­–ç•¥æ¡†æ¶ğ“Ÿğ“»ğ“¸

ç‰ˆæƒæ‰€æœ‰ Â©ï¸ é‚¢ä¸è¡Œ
å¾®ä¿¡: xbx1717

æœ¬ä»£ç ä»…ä¾›ä¸ªäººå­¦ä¹ ä½¿ç”¨ï¼Œæœªç»æˆæƒä¸å¾—å¤åˆ¶ã€ä¿®æ”¹æˆ–ç”¨äºå•†ä¸šç”¨é€”ã€‚

Author: é‚¢ä¸è¡Œ
"""

import sys
import time
from concurrent.futures import ProcessPoolExecutor
from pathlib import Path
from typing import Union, Callable, List

import numpy as np
import pandas as pd
from tqdm import tqdm

from config import n_jobs
from core.market_essentials import cal_fuquan_price, cal_zdt_price, merge_with_index_data
from core.model.backtest_config import BacktestConfig
from core.utils.data_hub import load_ext_data
from core.utils.log_kit import logger

# å®šä¹‰è‚¡ç¥¨æ•°æ®æ‰€éœ€çš„åˆ—
DATA_COLS = [
    "è‚¡ç¥¨ä»£ç ",
    "è‚¡ç¥¨åç§°",
    "äº¤æ˜“æ—¥æœŸ",
    "å¼€ç›˜ä»·",
    "æœ€é«˜ä»·",
    "æœ€ä½ä»·",
    "æ”¶ç›˜ä»·",
    "å‰æ”¶ç›˜ä»·",
    "æˆäº¤é‡",
    "æˆäº¤é¢",
    "æµé€šå¸‚å€¼",
    "æ€»å¸‚å€¼",
]


# ================================================================
# step1_æ•´ç†æ•°æ®.py
# ================================================================
def prepare_data(conf: BacktestConfig, boost: bool = True):
    logger.info(f"è¯»å–æ•°æ®ä¸­å¿ƒæ•°æ®...")
    start_time = time.time()  # è®°å½•æ•°æ®å‡†å¤‡å¼€å§‹æ—¶é—´

    # 0. å‡†å¤‡å·¥ä½œ
    if conf.ov_cols:
        logger.debug(f"ğŸ›‚ æ£€æµ‹åˆ°å› å­éœ€è¦é¢å¤–å…¨æ¯å­—æ®µï¼š{conf.ov_cols}")
    else:
        logger.debug("ğŸ›‚ æ²¡æœ‰å› å­éœ€è¦é¢å¤–çš„å…¨æ¯å­—æ®µ")

    stock_hint = "ä¸åŒ…æ‹¬"
    for board, name in [("kcb", "ç§‘åˆ›æ¿"), ("cyb", "åˆ›ä¸šæ¿"), ("bj", "åŒ—äº¤æ‰€")]:
        if board in conf.excluded_boards:
            logger.debug(f"ğŸ—‘ï¸ [ç­–ç•¥é…ç½®] éœ€è¦æ’é™¤`{name}`")
            stock_hint += f"{name}ã€"
    stock_hint = stock_hint.strip("ã€")

    if conf.rebalance_time_list:
        logger.debug(f"ğŸ•’ æ£€æµ‹åˆ°éœ€è¦åˆ†é’Ÿæ•°æ®ï¼š{conf.rebalance_time_list}ï¼Œéœ€è¦é¢å¤–å‡†å¤‡pivotæ•°æ®")
    else:
        logger.debug("ğŸ•’ æ²¡æœ‰åˆ†é’Ÿæ¢ä»“æ•°æ®è¦æ±‚")

    # 1. è·å–è‚¡ç¥¨ä»£ç åˆ—è¡¨
    stock_code_list = []  # ç”¨äºå­˜å‚¨è‚¡ç¥¨ä»£ç 
    # éå†æ–‡ä»¶å¤¹ä¸‹ï¼Œæ‰€æœ‰csvæ–‡ä»¶
    for filename in conf.stock_data_path.glob("*.csv"):
        # æ’é™¤éšè—æ–‡ä»¶
        if filename.stem.startswith("."):
            continue
        # åˆ¤æ–­æ˜¯å¦ä¸ºåŒ—äº¤æ‰€è‚¡ç¥¨ï¼ˆä»£ç ä»¥ 'bj' å¼€å¤´ï¼‰
        if filename.stem.startswith("bj") and ("bj" in conf.excluded_boards):
            continue
        # åˆ¤æ–­æ˜¯å¦ä¸ºç§‘åˆ›æ¿è‚¡ç¥¨ï¼ˆä»£ç ä»¥ 'sh68' å¼€å¤´ï¼‰
        if filename.stem.startswith("sh68") and ("kcb" in conf.excluded_boards):
            continue
        # åˆ¤æ–­æ˜¯å¦ä¸ºç§‘åˆ›æ¿è‚¡ç¥¨ï¼ˆä»£ç ä»¥ 'sz30' å¼€å¤´ï¼‰
        if filename.stem.startswith("sz30") and ("cyb" in conf.excluded_boards):
            continue

        stock_code_list.append(filename.stem)
    stock_code_list = sorted(stock_code_list)
    logger.debug(f"ğŸ“‚ è¯»å–åˆ°è‚¡ç¥¨æ•°é‡ï¼š{len(stock_code_list)}ï¼Œ{stock_hint}")

    # 2. è¯»å–å¹¶å¤„ç†æŒ‡æ•°æ•°æ®ï¼Œç¡®ä¿è‚¡ç¥¨æ•°æ®ä¸æŒ‡æ•°æ•°æ®çš„æ—¶é—´å¯¹é½
    index_data = conf.load_index_data()
    all_candle_data_dict = {}  # ç”¨äºå­˜å‚¨æ‰€æœ‰è‚¡ç¥¨çš„Kçº¿æ•°æ®

    logger.debug(f"ğŸš€ å¤šè¿›ç¨‹å¤„ç†æ•°æ®ï¼Œè¿›ç¨‹æ•°é‡ï¼š{n_jobs}" if boost else "ğŸš² å•è¿›ç¨‹å¤„ç†æ•°æ®")
    if boost:
        with ProcessPoolExecutor(max_workers=n_jobs) as executor:
            futures = []
            for code in stock_code_list:
                file_path = conf.stock_data_path / f"{code}.csv"
                futures.append(executor.submit(prepare_data_by_stock, conf, file_path, index_data))

            for future in tqdm(futures, desc="ğŸ“¦ å¤„ç†æ•°æ®", total=len(futures), mininterval=2, file=sys.stdout):
                df = future.result()
                if not df.empty:
                    code = df["è‚¡ç¥¨ä»£ç "].iloc[0]
                    all_candle_data_dict[code] = df  # ä»…å­˜å‚¨éç©ºæ•°æ®
    else:
        for code in tqdm(
            stock_code_list, desc="ğŸ“¦ å¤„ç†æ•°æ®", total=len(stock_code_list), mininterval=2, file=sys.stdout
        ):
            file_path = conf.stock_data_path / f"{code}.csv"
            df = prepare_data_by_stock(conf, file_path, index_data)
            if not df.empty:
                all_candle_data_dict[code] = df

    # è·å–æ‰€æœ‰è‚¡ç¥¨æ•°æ®çš„æœ€å¤§æ—¥æœŸ
    max_candle_date = max([df["äº¤æ˜“æ—¥æœŸ"].max() for df in all_candle_data_dict.values()])

    # 3. ç¼“å­˜é¢„å¤„ç†åçš„æ•°æ®
    cache_path = conf.get_runtime_folder() / "è‚¡ç¥¨é¢„å¤„ç†æ•°æ®.pkl"
    logger.debug(f"ğŸ“ˆ ä¿å­˜è‚¡ç¥¨é¢„å¤„ç†æ•°æ®: {cache_path}")
    logger.debug(f"ğŸ“… è¡Œæƒ…æ•°æ®æœ€æ–°äº¤æ˜“æ—¥æœŸï¼š{max_candle_date}")
    pd.to_pickle(all_candle_data_dict, cache_path)

    # 4. å‡†å¤‡å¹¶ç¼“å­˜pivoté€è§†è¡¨æ•°æ®ï¼Œç”¨äºåç»­å›æµ‹
    logger.debug("ğŸ“„ ç”Ÿæˆè¡Œæƒ…æ•°æ®é€è§†è¡¨...")
    market_pivot_dict = make_market_pivot(all_candle_data_dict, conf.rebalance_time_list)
    pivot_cache_path = conf.get_runtime_folder() / "å…¨éƒ¨è‚¡ç¥¨è¡Œæƒ…pivot.pkl"
    logger.debug(f"ğŸ—„ï¸ ä¿å­˜è¡Œæƒ…æ•°æ®é€è§†è¡¨: {pivot_cache_path}")
    pd.to_pickle(market_pivot_dict, pivot_cache_path)

    logger.ok(f"æ•°æ®å‡†å¤‡è€—æ—¶ï¼š{(time.time() - start_time):.2f} ç§’")


def prepare_data_by_stock(
    conf: BacktestConfig, stock_file_path: Union[str, Path], index_data: pd.DataFrame
) -> pd.DataFrame:
    """
    å¯¹è‚¡ç¥¨æ•°æ®è¿›è¡Œé¢„å¤„ç†ï¼ŒåŒ…æ‹¬åˆå¹¶æŒ‡æ•°æ•°æ®å’Œè®¡ç®—æœªæ¥äº¤æ˜“æ—¥çŠ¶æ€ã€‚

    å‚æ•°:
    stock_file_path (str | Path): è‚¡ç¥¨æ—¥çº¿æ•°æ®çš„è·¯å¾„
    index_data (DataFrame): æŒ‡æ•°æ•°æ®
    conf (BacktestConfig): ç³»ç»Ÿé…ç½®

    è¿”å›:
    df1 (DataFrame): é¢„å¤„ç†åçš„æ•°æ®
    df2 (DataFrame): åˆ†é’Ÿä»·æ ¼æ•°æ®
    """
    # è®¡ç®—æ¶¨è·Œå¹…ã€æ¢æ‰‹ç‡ç­‰å…³é”®æŒ‡æ ‡
    df = pd.read_csv(
        stock_file_path, encoding="gbk", skiprows=1, parse_dates=["äº¤æ˜“æ—¥æœŸ"], usecols=DATA_COLS + conf.ov_cols
    )
    pct_change = df["æ”¶ç›˜ä»·"] / df["å‰æ”¶ç›˜ä»·"] - 1
    turnover_rate = df["æˆäº¤é¢"] / df["æµé€šå¸‚å€¼"]
    trading_days = df.index.astype("int") + 1
    avg_price = df["æˆäº¤é¢"] / df["æˆäº¤é‡"]

    # ä¸€æ¬¡æ€§èµ‹å€¼æé«˜æ€§èƒ½
    df = df.assign(æ¶¨è·Œå¹…=pct_change, æ¢æ‰‹ç‡=turnover_rate, ä¸Šå¸‚è‡³ä»Šäº¤æ˜“å¤©æ•°=trading_days, å‡ä»·=avg_price)

    # å¤æƒä»·è®¡ç®—åŠæ¶¨è·Œåœä»·æ ¼è®¡ç®—
    df = cal_fuquan_price(df, fuquan_type="åå¤æƒ")
    df = cal_zdt_price(df)

    # åˆå¹¶è‚¡ç¥¨ä¸æŒ‡æ•°æ•°æ®ï¼Œè¡¥å…¨åœç‰Œæ—¥æœŸç­‰ä¿¡æ¯
    df = merge_with_index_data(df, index_data.copy(), fill_0_list=["æ¢æ‰‹ç‡"])

    # è‚¡ç¥¨é€€å¸‚æ—¶é—´å°äºæŒ‡æ•°å¼€å§‹æ—¶é—´ï¼Œå°±ä¼šå‡ºç°ç©ºå€¼
    if df.empty:
        # å¦‚æœå‡ºç°è¿™ç§æƒ…å†µï¼Œè¿”å›ç©ºçš„DataFrameç”¨äºåç»­æ“ä½œ
        return pd.DataFrame(columns=[*DATA_COLS, *conf.rebalance_time_list])

    # å¦‚æœå›æµ‹ç”¨åˆ°åˆ†é’Ÿæ•°æ®ï¼Œè¿˜éœ€è¦å¤–è¯»å–åˆ†é’Ÿæ˜¯æ•°æ®
    if conf.rebalance_time_list:
        df = load_min_data(conf, df)

    # è®¡ç®—å¼€ç›˜ä¹°å…¥æ¶¨è·Œå¹…å’Œæœªæ¥äº¤æ˜“æ—¥çŠ¶æ€
    df = df.assign(
        ä¸‹æ—¥_æ˜¯å¦äº¤æ˜“=df["æ˜¯å¦äº¤æ˜“"].astype("int8").shift(-1),
        ä¸‹æ—¥_ä¸€å­—æ¶¨åœ=df["ä¸€å­—æ¶¨åœ"].astype("int8").shift(-1),
        ä¸‹æ—¥_å¼€ç›˜æ¶¨åœ=df["å¼€ç›˜æ¶¨åœ"].astype("int8").shift(-1),
        ä¸‹æ—¥_æ˜¯å¦ST=df["è‚¡ç¥¨åç§°"].str.contains("ST").astype("int8").shift(-1),
        ä¸‹æ—¥_æ˜¯å¦S=df["è‚¡ç¥¨åç§°"].str.contains("S").astype("int8").shift(-1),
        ä¸‹æ—¥_æ˜¯å¦é€€å¸‚=df["è‚¡ç¥¨åç§°"].str.contains("é€€").astype("int8").shift(-1),
    )

    # å¤„ç†æœ€åä¸€æ ¹Kçº¿çš„æ•°æ®ï¼šæœ€åä¸€æ ¹Kçº¿é»˜è®¤æ²¿ç”¨å‰ä¸€æ—¥çš„æ•°æ®
    state_cols = ["ä¸‹æ—¥_æ˜¯å¦äº¤æ˜“", "ä¸‹æ—¥_æ˜¯å¦ST", "ä¸‹æ—¥_æ˜¯å¦S", "ä¸‹æ—¥_æ˜¯å¦é€€å¸‚"]
    df[state_cols] = df[state_cols].ffill()

    # æ¸…ç†é€€å¸‚æ•°æ®ï¼Œä¿ç•™æœ‰æ•ˆäº¤æ˜“æ•°æ®
    if ("é€€" in df["è‚¡ç¥¨åç§°"].iloc[-1]) or ("S" in df["è‚¡ç¥¨åç§°"].iloc[-1]):
        if df["æˆäº¤é¢"].iloc[-1] == 0 and np.all(df["æˆäº¤é¢"] == 0):
            return pd.DataFrame(columns=[*DATA_COLS, *conf.rebalance_time_list])
        # @é©¬è¶… åŒå­¦äº2024å¹´11æœˆ20æ—¥æä¾›é€€å¸‚é€»è¾‘ä¼˜åŒ–å¤„ç†ã€‚
        # è§£å†³å› ä¸ºèµ·å§‹æ—¶é—´å¤ªé å‰ï¼Œå¯¼è‡´æ•°æ®å¯èƒ½ä¸ºç©ºæŠ¥é”™çš„é—®é¢˜ï¼ŒåŠ å…¥äº†emptyæƒ…å†µçš„å®¹é”™
        df_tmp = df[(df["æˆäº¤é¢"] != 0) & (df["æˆäº¤é¢"].shift(-1) == 0)]
        if df_tmp.empty:
            end_date = df["äº¤æ˜“æ—¥æœŸ"].iloc[-1]
        else:
            end_date = df_tmp.iloc[-1]["äº¤æ˜“æ—¥æœŸ"]
        df = df[df["äº¤æ˜“æ—¥æœŸ"] <= end_date]

    return df


def load_min_data(conf: BacktestConfig, df):
    """
    åŠ è½½åˆ†é’Ÿæ•°æ®
    :param df: åŸå§‹çš„Kçº¿æ•°æ®
    :param conf: ç³»ç»Ÿé…ç½®
    :return:
    """
    match conf.min_data_level:
        case "5m":
            df = merge_extra_data(df, "5min_close", conf.rebalance_time_list)
        case "15m":
            df = merge_extra_data(df, "15min_close", conf.rebalance_time_list)
        case _:
            return df
    # åœç‰Œçš„æ—¶å€™ä½¿ç”¨æ”¶ç›˜ä»·å¡«å……
    for reb_time in conf.rebalance_time_list:
        df[reb_time] = df[reb_time].fillna(df["æ”¶ç›˜ä»·"])
    return df


def make_market_pivot(market_dict, rebalance_time_list):
    """
    æ„å»ºå¸‚åœºæ•°æ®çš„pivoté€è§†è¡¨ï¼Œä¾¿äºå›æµ‹è®¡ç®—ã€‚

    å‚æ•°:
    market_dict (dict): è‚¡ç¥¨Kçº¿æ•°æ®å­—å…¸
    rebalance_time_list (list):åˆ†é’Ÿæ•°æ®çš„å­—æ®µåˆ—è¡¨

    è¿”å›:
    dict: åŒ…å«å¼€ç›˜ä»·ã€æ”¶ç›˜ä»·åŠå‰æ”¶ç›˜ä»·çš„é€è§†è¡¨æ•°æ®
    """
    cols = ["äº¤æ˜“æ—¥æœŸ", "è‚¡ç¥¨ä»£ç ", "å¼€ç›˜ä»·", "æ”¶ç›˜ä»·", "å‰æ”¶ç›˜ä»·", *rebalance_time_list]
    counts = 3 + len(rebalance_time_list)
    count = 1

    logger.debug("âš—ï¸ åˆæˆæ•´ä½“å¸‚åœºæ•°æ®...")
    df_list = [df[cols].dropna(subset="è‚¡ç¥¨ä»£ç ") for df in market_dict.values()]
    df_all_market = pd.concat(df_list, ignore_index=True)
    col_names = {"å¼€ç›˜ä»·": "open", "æ”¶ç›˜ä»·": "close", "å‰æ”¶ç›˜ä»·": "preclose"}

    markets = {}
    for col in cols[2:]:
        logger.debug(f"[{count}/{counts}] {col}é€è§†è¡¨...")
        df_col = df_all_market.pivot(values=col, index="äº¤æ˜“æ—¥æœŸ", columns="è‚¡ç¥¨ä»£ç ")
        markets[col_names.get(col, col)] = df_col
        count += 1

    return markets


# ===============================================================================================================
# é¢å¤–æ•°æ®æº
# ===============================================================================================================
def merge_extra_data(df: pd.DataFrame, data_name: str, save_cols: List[str]) -> pd.DataFrame:
    """
    å¯¼å…¥æ•°æ®ï¼Œæœ€ç»ˆåªè¿”å›å¸¦æœ‰åŒindexçš„æ•°æ®
    :param df: ï¼ˆåªè¯»ï¼‰åŸå§‹çš„è¡Œæƒ…æ•°æ®ï¼Œä¸»è¦æ˜¯å¯¹é½æ•°æ®ç”¨çš„
    :param data_name: æ•°æ®ä¸­å¿ƒä¸­çš„æ•°æ®è‹±æ–‡å
    :param save_cols: éœ€è¦ä¿å­˜çš„åˆ—
    :return: åˆå¹¶åçš„æ•°æ®
    """
    import core.data_bridge as db

    ext_data_dict = load_ext_data()
    if ext_data_dict:
        logger.debug(f"ğŸ” æ£€æµ‹åˆ°å¤–éƒ¨æ•°æ®æºï¼š{ext_data_dict.keys()}ï¼Œå¯ä»¥åœ¨ç­–ç•¥é…ç½®ä¸­è®¢é˜…ä½¿ç”¨")
    data_source_dict = {**db.presets, **ext_data_dict}

    func_name, file_path = data_source_dict[data_name]

    if isinstance(func_name, Callable):
        func = func_name
    elif hasattr(db, func_name):
        func = getattr(db, func_name)
    else:
        print(f"âš ï¸ æœªå®ç°æ•°æ®æºï¼š{data_name}")
        return df.assign(**{col: np.nan for col in save_cols})
    try:
        extra_df = func(file_path, df, save_cols)
    except Exception as e:
        raise e

    if extra_df is None or extra_df.empty:
        return df.assign(**{col: np.nan for col in save_cols})

    return extra_df


def check_extra_data(data_name: str):
    """
    æ•°æ®é¢„æ£€æŸ¥
    """
    import core.data_bridge as db

    ext_data_dict = load_ext_data()
    if ext_data_dict:
        logger.debug(f"ğŸ” æ£€æµ‹åˆ°å¤–éƒ¨æ•°æ®æºï¼š{ext_data_dict.keys()}ï¼Œå¯ä»¥åœ¨ç­–ç•¥é…ç½®ä¸­è®¢é˜…ä½¿ç”¨")
    data_source_dict = {**db.presets, **ext_data_dict}

    func_name, file_path = data_source_dict[data_name]

    file_path = Path(file_path)
    if not file_path.exists():
        return False, f"æ–‡ä»¶ä¸å­˜åœ¨ï¼š{file_path}ï¼Œè¯·åœ¨æ•°æ®ä¸­å¿ƒè®¢é˜…æˆ–æ‰‹åŠ¨ä¸‹è½½åé‡è¯•"

    if isinstance(func_name, Callable):
        return True, "OK"

    fail_msg = f"âš ï¸ æœªå®ç°æ•°æ®æºï¼š{data_name}"
    return hasattr(db, func_name), fail_msg
