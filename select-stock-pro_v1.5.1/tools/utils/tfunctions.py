import gc

import pandas as pd
import numpy as np
import hashlib
import os
from pathlib import Path
from tqdm import tqdm
import datetime
from concurrent.futures import ProcessPoolExecutor
from core.fin_essentials import merge_with_finance_data
from core.utils.path_kit import get_folder_path
import math


# region é€šç”¨å‡½æ•°
def get_data_path_md5(data_path):
    # å°†æ–‡ä»¶å¤¹çš„å¤§å°ã€æ›´æ”¹æ—¶é—´ä¿¡æ¯ä½œä¸ºå‚æ•°ï¼Œç”Ÿæˆmd5å€¼
    info_txt = ''
    if data_path.is_dir():
        files = list(sorted([str(f) for f in data_path.iterdir() if f.is_file()]))
        for file in files:
            info_txt += f'{os.path.getsize(file)}-{os.path.getmtime(file)}'
    if data_path.is_file():
        info_txt += f'{os.path.getsize(data_path)}-{os.path.getmtime(data_path)}'
    md5_txt = hashlib.md5(info_txt.encode('utf-8')).hexdigest()
    return md5_txt


def read_txt(path):
    with open(path, 'r') as f:
        txt = f.read()
    return txt


def write_txt(path, txt):
    with open(path, 'w') as f:
        f.write(txt)


def filter_stock(df):
    """
    è¿‡æ»¤å‡½æ•°ï¼ŒST/é€€å¸‚/äº¤æ˜“å¤©æ•°ä¸è¶³ç­‰æƒ…å†µ
    :param df:
    :return:
    """
    # =åˆ é™¤ä¸èƒ½äº¤æ˜“çš„å‘¨æœŸæ•°
    # åˆ é™¤æœˆæœ«ä¸ºstçŠ¶æ€çš„å‘¨æœŸæ•°
    df = df[df['è‚¡ç¥¨åç§°'].str.contains('ST') == False]
    # åˆ é™¤æœˆæœ«ä¸ºsçŠ¶æ€çš„å‘¨æœŸæ•°
    df = df[df['è‚¡ç¥¨åç§°'].str.contains('S') == False]
    # åˆ é™¤æœˆæœ«æœ‰é€€å¸‚é£Žé™©çš„å‘¨æœŸæ•°
    df = df[df['è‚¡ç¥¨åç§°'].str.contains('\*') == False]
    df = df[df['è‚¡ç¥¨åç§°'].str.contains('é€€') == False]

    df = df[df['ä¸‹æ—¥_æ˜¯å¦äº¤æ˜“'] == 1]
    df = df[df['ä¸‹æ—¥_å¼€ç›˜æ¶¨åœ'] == False]
    df = df[df['ä¸‹æ—¥_æ˜¯å¦ST'] == False]
    df = df[df['ä¸‹æ—¥_æ˜¯å¦é€€å¸‚'] == False]
    df = df[df['ä¸Šå¸‚è‡³ä»Šäº¤æ˜“å¤©æ•°'] > 250]

    return df


def float_num_process(num, return_type=float, keep=2, max=5):
    """
    é’ˆå¯¹ç»å¯¹å€¼å°äºŽ1çš„æ•°å­—è¿›è¡Œç‰¹æ®Šå¤„ç†ï¼Œä¿ç•™éž0çš„Nä½ï¼ˆNé»˜è®¤ä¸º2ï¼Œå³keepå‚æ•°ï¼‰
    è¾“å…¥  0.231  è¾“å‡º  0.23
    è¾“å…¥  0.0231  è¾“å‡º  0.023
    è¾“å…¥  0.00231  è¾“å‡º  0.0023
    å¦‚æžœå‰é¢maxä¸ªéƒ½æ˜¯0ï¼Œç›´æŽ¥è¿”å›ž0.0
    :param num: è¾“å…¥çš„æ•°æ®
    :param return_type: è¿”å›žçš„æ•°æ®ç±»åž‹ï¼Œé»˜è®¤æ˜¯float
    :param keep: éœ€è¦ä¿ç•™çš„éžé›¶ä½æ•°
    :param max: æœ€é•¿ä¿ç•™å¤šå°‘ä½
    :return:
        è¿”å›žä¸€ä¸ªfloatæˆ–str
    """

    # å¦‚æžœè¾“å…¥çš„æ•°æ®æ˜¯0ï¼Œç›´æŽ¥è¿”å›ž0.0
    if num == 0.:
        return 0.0

    # ç»å¯¹å€¼å¤§äºŽ1çš„æ•°ç›´æŽ¥ä¿ç•™å¯¹åº”çš„ä½æ•°è¾“å‡º
    if abs(num) > 1:
        return round(num, keep)
    # èŽ·å–å°æ•°ç‚¹åŽé¢æœ‰å¤šå°‘ä¸ª0
    zero_count = -int(math.log10(abs(num)))
    # å®žé™…éœ€è¦ä¿ç•™çš„ä½æ•°
    keep = min(zero_count + keep, max)

    # å¦‚æžœæŒ‡å®šreturn_typeæ˜¯floatï¼Œåˆ™è¿”å›žfloatç±»åž‹çš„æ•°æ®
    if return_type == float:
        return round(num, keep)
    # å¦‚æžœæŒ‡å®šreturn_typeæ˜¯strï¼Œåˆ™è¿”å›žstrç±»åž‹çš„æ•°æ®
    else:
        return str(round(num, keep))


# endregion

# region å•å› å­åˆ†æžè¦ç”¨åˆ°çš„å‡½æ•°ï¼ˆä¸åˆ†åŒå› å­çš„ä¹Ÿåœ¨ï¼‰

def process_stock(stock_folder, per_df, cfg, stock):
    stock_path = stock_folder / stock
    df = pd.read_csv(stock_path, encoding='gbk', parse_dates=['äº¤æ˜“æ—¥æœŸ'], skiprows=1)
    max_date = df['äº¤æ˜“æ—¥æœŸ'].max()
    min_date = df['äº¤æ˜“æ—¥æœŸ'].min()
    df = pd.merge(df, per_df[per_df['äº¤æ˜“æ—¥æœŸ'].between(min_date, max_date)], on='äº¤æ˜“æ—¥æœŸ', how='right')
    if df.empty:
        return pd.DataFrame()
    for col in ['è‚¡ç¥¨ä»£ç ', 'äº¤æ˜“æ—¥æœŸ', 'æ”¶ç›˜ä»·', 'æ€»å¸‚å€¼', 'æ–°ç‰ˆç”³ä¸‡ä¸€çº§è¡Œä¸šåç§°']:
        df[col] = df[col].ffill()

    df['æ¶¨è·Œå¹…'] = df['æ”¶ç›˜ä»·'] / df['å‰æ”¶ç›˜ä»·'] - 1
    df['æ¶¨è·Œå¹…'] = df['æ¶¨è·Œå¹…'].fillna(value=0)
    df['å¼€ç›˜ä»·'] = df['å¼€ç›˜ä»·'].fillna(value=df['æ”¶ç›˜ä»·'])
    # è®¡ç®—å¤æƒå› å­
    df['å¤æƒå› å­'] = (df['æ¶¨è·Œå¹…'] + 1).cumprod()
    df['æ”¶ç›˜ä»·_å¤æƒ'] = df['å¤æƒå› å­'] * (df.iloc[0]['æ”¶ç›˜ä»·'] / df['å¤æƒå› å­'].iloc[0])
    df['å¼€ç›˜ä»·_å¤æƒ'] = df['å¼€ç›˜ä»·'] / df['æ”¶ç›˜ä»·'] * df['æ”¶ç›˜ä»·_å¤æƒ']

    # è®¡ç®—é£Žæ ¼å› å­
    fin_cols = ['R_np@xbx_ttm', 'B_total_equity_atoopc@xbx', 'R_revenue@xbx_ttm', 'R_np@xbx_ttmåŒæ¯”',
                'R_revenue@xbx_ttmåŒæ¯”', 'R_np@xbx_å•å­£åŒæ¯”', 'R_revenue@xbx_å•å­£åŒæ¯”', 'B_total_liab@xbx',
                'B_actual_received_capital@xbx', 'B_preferred_shares@xbx', 'B_total_assets@xbx',
                'B_total_liab_and_owner_equity@xbx', 'R_op@xbx_ttm']
    cfg.fin_cols = fin_cols
    df = merge_with_finance_data(cfg, stock[:-4], df)[0]

    name = 'é£Žæ ¼å› å­_'
    # ===ä¼°å€¼å› å­
    df[name + 'EP'] = df['R_np@xbx_ttm'] / df['æ€»å¸‚å€¼']  # å¸‚ç›ˆçŽ‡å€’æ•°
    df[name + 'BP'] = df['B_total_equity_atoopc@xbx'] / df['æ€»å¸‚å€¼']  # å¸‚å‡€çŽ‡å€’æ•°
    df[name + 'SP'] = df['R_revenue@xbx_ttm'] / df['æ€»å¸‚å€¼']  # å¸‚é”€çŽ‡å€’æ•°

    # ===åŠ¨é‡å› å­
    df[name + 'Ret_252'] = df['æ”¶ç›˜ä»·_å¤æƒ'].shift(21) / df['æ”¶ç›˜ä»·_å¤æƒ'].shift(252) - 1

    # ===åè½¬å› å­
    df[name + 'Ret_21'] = df['æ”¶ç›˜ä»·_å¤æƒ'] / df['æ”¶ç›˜ä»·_å¤æƒ'].shift(21) - 1

    # ===æˆé•¿å› å­
    df[name + 'å‡€åˆ©æ¶¦ttmåŒæ¯”'] = df['R_np@xbx_ttmåŒæ¯”']
    df[name + 'è¥ä¸šæ”¶å…¥ttmåŒæ¯”'] = df['R_revenue@xbx_ttmåŒæ¯”']
    df[name + 'å‡€åˆ©æ¶¦å•å­£åŒæ¯”'] = df['R_np@xbx_å•å­£åŒæ¯”']
    df[name + 'è¥ä¸šæ”¶å…¥å•å­£åŒæ¯”'] = df['R_revenue@xbx_å•å­£åŒæ¯”']

    # ===æ æ†å› å­
    df[name + 'MLEV'] = (df['æ€»å¸‚å€¼'] + df['B_total_liab@xbx']) / df['æ€»å¸‚å€¼']
    df[name + 'BLEV'] = (df[['B_actual_received_capital@xbx', 'B_preferred_shares@xbx']].sum(axis=1, skipna=True)) / df[
        'æ€»å¸‚å€¼']
    df[name + 'DTOA'] = df['B_total_liab@xbx'] / df['B_total_assets@xbx']

    # ===æ³¢åŠ¨å› å­
    df[name + 'Std21'] = df['æ¶¨è·Œå¹…'].rolling(21).std()
    df[name + 'Std252'] = df['æ¶¨è·Œå¹…'].rolling(252).std()

    # ===ç›ˆåˆ©å› å­
    df[name + 'ROE'] = df['R_np@xbx_ttm'] / df['B_total_equity_atoopc@xbx']  # ROE å‡€èµ„äº§æ”¶ç›ŠçŽ‡
    df[name + 'ROA'] = df['R_np@xbx_ttm'] / df['B_total_liab_and_owner_equity@xbx']  # ROA èµ„äº§æ”¶ç›ŠçŽ‡
    df[name + 'å‡€åˆ©æ¶¦çŽ‡'] = df['R_np@xbx_ttm'] / df['R_revenue@xbx_ttm']  # å‡€åˆ©æ¶¦çŽ‡ï¼šå‡€åˆ©æ¶¦ / è¥ä¸šæ”¶å…¥
    df[name + 'GP'] = df['R_op@xbx_ttm'] / df['B_total_assets@xbx']

    # ===è§„æ¨¡å› å­
    df[name + 'æ€»å¸‚å€¼'] = np.log(df['æ€»å¸‚å€¼'])

    # åšä¸€äº›ç®€å•çš„å‘¨æœŸè½¬æ¢
    agg_dict = {'äº¤æ˜“æ—¥æœŸ': 'last', 'è‚¡ç¥¨ä»£ç ': 'last', 'å¼€ç›˜ä»·_å¤æƒ': 'first',
                'æ”¶ç›˜ä»·_å¤æƒ': 'last', 'æ–°ç‰ˆç”³ä¸‡ä¸€çº§è¡Œä¸šåç§°': 'last'}
    style_cols = [col for col in df.columns if col.startswith(name)]
    for col in style_cols:
        agg_dict[col] = 'last'
    period_df = df.groupby(cfg.period_offset).agg(agg_dict)

    # è®¡ç®—ä¸‹å‘¨æœŸçš„æ”¶ç›Š
    period_df['ä¸‹å‘¨æœŸæ¶¨è·Œå¹…'] = (period_df['æ”¶ç›˜ä»·_å¤æƒ'] / period_df['å¼€ç›˜ä»·_å¤æƒ'] - 1).shift(-1)
    # è®¡ç®—ä¸‹å‘¨æœŸæ¯å¤©çš„æ”¶ç›Š
    period_df['ä¸‹å‘¨æœŸæ¯å¤©æ¶¨è·Œå¹…'] = df.groupby(cfg.period_offset)['æ¶¨è·Œå¹…'].apply(lambda x: list(x)).shift(-1)

    period_df.dropna(subset=['ä¸‹å‘¨æœŸæ¶¨è·Œå¹…', 'ä¸‹å‘¨æœŸæ¯å¤©æ¶¨è·Œå¹…'], how='any', inplace=True)
    return period_df


def get_data(cfg, _factor_list, boost):
    # èŽ·å–æœªæ¥æ¶¨è·Œå¹…æ•°æ®
    rs_df = get_ret_and_style(cfg, boost)

    # è¯»å–å› å­æ•°æ®
    factor_df = pd.read_pickle(cfg.get_result_folder().parent.parent / 'è¿è¡Œç¼“å­˜/all_factors_kline.pkl')

    for factor_name in _factor_list:
        factor = pd.read_pickle(cfg.get_result_folder().parent.parent / f'è¿è¡Œç¼“å­˜/{factor_name}.pkl')
        if factor.empty:
            raise ValueError(f"{factor} å› å­æ•°æ®ä¸ºç©ºï¼Œè¯·æ£€æŸ¥æ•°æ®")
        if len(factor_df) != len(factor):
            raise ValueError(f"{factor} å› å­é•¿åº¦ä¸åŒ¹é…ï¼Œéœ€è¦é‡æ–°å›žæµ‹ï¼Œæ›´æ–°æ•°æ®")
        factor_df[factor_name] = factor

    factor_df = pd.merge(factor_df, rs_df, on=['äº¤æ˜“æ—¥æœŸ', 'è‚¡ç¥¨ä»£ç '], how='right')
    # æ•°æ®æ¸…æ´—
    factor_df = data_preprocess(factor_df, cfg)
    if factor_df.empty:
        return pd.DataFrame()
    drop_cols = ['ä¸Šå¸‚è‡³ä»Šäº¤æ˜“å¤©æ•°', 'å¤æƒå› å­', 'å¼€ç›˜ä»·', 'æœ€é«˜ä»·', 'æœ€ä½Žä»·', 'æ”¶ç›˜ä»·', 'æˆäº¤é¢', 'æ˜¯å¦äº¤æ˜“',
                 'ä¸‹æ—¥_å¼€ç›˜æ¶¨åœ', 'ä¸‹æ—¥_æ˜¯å¦ST', 'ä¸‹æ—¥_æ˜¯å¦äº¤æ˜“', 'ä¸‹æ—¥_æ˜¯å¦é€€å¸‚']
    factor_df.drop(columns=drop_cols, inplace=True)
    del rs_df, drop_cols
    gc.collect()
    return factor_df


def cal_style_factor(df, style_name, base_factors):
    print(f'å¼€å§‹è®¡ç®—ã€{style_name}ã€‘é£Žæ ¼å› å­...')
    name = 'é£Žæ ¼å› å­_'
    factor_cols = []
    for factor in base_factors:
        if not factor.startswith(name):
            factor = name + factor
        df[factor] = df.groupby('äº¤æ˜“æ—¥æœŸ')[factor].rank(ascending=True, method='min')
        factor_cols.append(factor)

    df[name + style_name] = df[factor_cols].sum(axis=1)
    df.drop(columns=factor_cols, inplace=True)
    return df


def get_ret_and_style(cfg, boost):
    cal_future_rate = False
    # çœ‹ä¸€ä¸‹md5æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    md5_file = get_folder_path(cfg.get_result_folder().parent.parent, 'è¿è¡Œç¼“å­˜', 'md5ä¿¡æ¯') / 'future_rate_md5.txt'
    stock_folder = cfg.stock_data_path
    future_rate_path = get_folder_path(cfg.get_result_folder().parent.parent, 'è¿è¡Œç¼“å­˜') / 'æœªæ¥æ”¶ç›ŠåŠé£Žæ ¼å› å­.pkl'
    if (not md5_file.exists()) or not (future_rate_path.exists()):
        cal_future_rate = True
        new_md5_txt = get_data_path_md5(stock_folder)
    else:
        old_md5_txt = read_txt(md5_file)
        new_md5_txt = get_data_path_md5(stock_folder)
        if old_md5_txt != new_md5_txt:
            cal_future_rate = True
    if cal_future_rate:
        print('æ•°æ®å‘ç”Ÿå˜æ›´ï¼Œéœ€è¦é‡æ–°è®¡ç®—æœªæ¥æ”¶ç›Š & é£Žæ ¼å› å­')
        start_time = datetime.datetime.now()  # è®°å½•å¼€å§‹æ—¶é—´
        if 'M' in cfg.period_offset:
            raise ValueError('å› å­åˆ†æžä¸æ”¯æŒMç³»åˆ—çš„offsetï¼Œå› ä¸ºæ¯æœˆçš„äº¤æ˜“æ—¥æ•°ä¸å›ºå®š')
        if cfg.period_offset == 'W53_0':
            raise ValueError('å› å­åˆ†æžä¸æ”¯æŒW53_0çš„offset')
        # è¯»å–period_offsetæ•°æ®
        period_offset_df = pd.read_csv(Path(cfg.data_center_path) / 'period_offset.csv', encoding='gbk', skiprows=1,
                                       parse_dates=['äº¤æ˜“æ—¥æœŸ'], usecols=['äº¤æ˜“æ—¥æœŸ', cfg.period_offset])
        dfs = []
        stock_list = [s for s in os.listdir(stock_folder) if ('.csv' in s) and ('bj' not in s)]
        if boost:
            with ProcessPoolExecutor(max_workers=os.cpu_count() - 1) as executor:
                futures = []
                for code in stock_list:
                    futures.append(executor.submit(process_stock, stock_folder, period_offset_df, cfg, code))
                for future in tqdm(futures, desc='ðŸ“¦ å¤„ç†æ•°æ®', total=len(futures)):
                    df = future.result()
                    dfs.append(df)
        else:
            for stock in tqdm(stock_list):
                dfs.append(process_stock(stock_folder, period_offset_df, cfg, stock))

        df = pd.concat(dfs, ignore_index=True)
        del dfs
        gc.collect()

        # å¯¹é£Žæ ¼å› å­åšæˆªé¢å¤„ç†
        # ===ä¼°å€¼
        df = cal_style_factor(df, 'ä¼°å€¼', ['EP', 'BP', 'SP'])

        # ===åŠ¨é‡
        df = cal_style_factor(df, 'åŠ¨é‡', ['Ret_252'])

        # ===åè½¬
        df = cal_style_factor(df, 'åè½¬', ['Ret_21'])

        # ===æˆé•¿
        df = cal_style_factor(df, 'æˆé•¿', ['å‡€åˆ©æ¶¦ttmåŒæ¯”', 'è¥ä¸šæ”¶å…¥ttmåŒæ¯”', 'å‡€åˆ©æ¶¦å•å­£åŒæ¯”', 'è¥ä¸šæ”¶å…¥å•å­£åŒæ¯”'])

        # ===æ æ†
        df = cal_style_factor(df, 'æ æ†', ['MLEV', 'BLEV', 'DTOA'])

        # ===æ³¢åŠ¨
        df = cal_style_factor(df, 'æ³¢åŠ¨', ['Std21', 'Std252'])

        # ===ç›ˆåˆ©
        df = cal_style_factor(df, 'ç›ˆåˆ©', ['ROE', 'ROA', 'å‡€åˆ©æ¶¦çŽ‡', 'GP'])

        # ===è§„æ¨¡
        df = cal_style_factor(df, 'è§„æ¨¡', ['æ€»å¸‚å€¼'])

        df.to_pickle(future_rate_path)
        write_txt(md5_file, new_md5_txt)
        print(f'è®¡ç®—è€—æ—¶ï¼š{datetime.datetime.now() - start_time}')
    else:
        print('æ•°æ®æ— å˜æ›´ï¼Œæ— éœ€é‡æ–°è®¡ç®—æœªæ¥æ”¶ç›Š')
        start_time = datetime.datetime.now()  # è®°å½•å¼€å§‹æ—¶é—´
        write_txt(md5_file, new_md5_txt)
        df = pd.read_pickle(future_rate_path)
        print(f'è¯»å–è€—æ—¶ï¼š{datetime.datetime.now() - start_time}')
    return df


def data_preprocess(df, cfg):
    # åˆ é™¤æ•°æ®ä¸å…¨çš„æ—¥æœŸ
    df.dropna(subset=['è‚¡ç¥¨ä»£ç '], inplace=True)
    # è¿‡æ»¤æŽ‰æ— æ³•äº¤æ˜“çš„è‚¡ç¥¨
    df = filter_stock(df)
    # é¢å¤–çš„è®¡ç®—å‡½æ•°
    df = cfg.func(df)
    # åˆ é™¤æŽ‰å­—æ®µä¸ºç©ºçš„åˆ—
    # åˆ é™¤å¿…è¦å­—æ®µä¸ºç©ºçš„éƒ¨åˆ†
    df = df.dropna(subset=cfg.keep_cols, how='any')
    # å°†å› å­ä¿¡æ¯è½¬æ¢æˆfloatç±»åž‹
    if hasattr(cfg, 'fa_name'):
        df[cfg.fa_name] = df[cfg.fa_name].astype(float)
    else:
        df[cfg.main] = df[cfg.main].astype(float)
        df[cfg.sub] = df[cfg.sub].astype(float)

    # =ä¿ç•™æ¯ä¸ªå‘¨æœŸçš„è‚¡ç¥¨æ•°é‡å¤§äºŽlimitçš„æ—¥æœŸ
    df['å½“å‘¨æœŸè‚¡ç¥¨æ•°'] = df.groupby('äº¤æ˜“æ—¥æœŸ')['äº¤æ˜“æ—¥æœŸ'].transform('count')
    df = df[df['å½“å‘¨æœŸè‚¡ç¥¨æ•°'] > cfg.limit].reset_index(drop=True)
    if df.empty:
        return df

    # å¦‚æžœæ˜¯å•å› å­åˆ†æž
    if hasattr(cfg, 'fa_name'):
        # æŒ‰ç…§å› å­å¯¹æ•°æ®è¿›è¡Œåˆ†ç»„
        df['å› å­_æŽ’å'] = df.groupby(['äº¤æ˜“æ—¥æœŸ'])[cfg.fa_name].rank(ascending=True, method='first')
        df['groups'] = df.groupby(['äº¤æ˜“æ—¥æœŸ'])['å› å­_æŽ’å'].transform(
            lambda x: pd.qcut(x, q=cfg.bins, labels=range(1, cfg.bins + 1), duplicates='drop'))
    # å¦‚æžœæ˜¯åŒå› å­åˆ†æž
    else:
        df = double_factor_grouping(df, cfg)
    return df


def get_ic(df, cfg):
    print('æ­£åœ¨è¿›è¡Œå› å­ICåˆ†æž...')
    start_time = datetime.datetime.now()  # è®°å½•å¼€å§‹æ—¶é—´

    # è®¡ç®—ICå¹¶å¤„ç†æ•°æ®
    ic = df.groupby('äº¤æ˜“æ—¥æœŸ').apply(lambda x: x[cfg.fa_name].corr(x['ä¸‹å‘¨æœŸæ¶¨è·Œå¹…'], method='spearman')).to_frame()
    ic = ic.rename(columns={0: 'RankIC'}).reset_index()
    ic['ç´¯è®¡RankIC'] = ic['RankIC'].cumsum()

    # ===è®¡ç®—ICçš„ç»Ÿè®¡å€¼ï¼Œå¹¶è¿›è¡Œçº¦ç­‰
    # =ICå‡å€¼
    ic_mean = float_num_process(ic['RankIC'].mean())
    # =ICæ ‡å‡†å·®
    ic_std = float_num_process(ic['RankIC'].std())
    # =icir
    icir = float_num_process(ic_mean / ic_std)
    # =ICèƒœçŽ‡
    # å¦‚æžœç´¯è®¡ICä¸ºæ­£ï¼Œåˆ™è®¡ç®—ICä¸ºæ­£çš„æ¯”ä¾‹
    if ic['ç´¯è®¡RankIC'].iloc[-1] > 0:
        ic_ratio = str(float_num_process((ic['RankIC'] > 0).sum() / len(ic) * 100)) + '%'
    # å¦‚æžœç´¯è®¡ICä¸ºè´Ÿï¼Œåˆ™è®¡ç®—ICä¸ºè´Ÿçš„æ¯”ä¾‹
    else:
        ic_ratio = str(float_num_process((ic['RankIC'] < 0).sum() / len(ic) * 100)) + '%'

    # å°†ä¸Šè¿°æŒ‡æ ‡åˆæˆä¸€ä¸ªå­—ç¬¦ä¸²ï¼ŒåŠ å…¥åˆ°ICå›¾ä¸­
    ic_info = f'ICå‡å€¼ï¼š{ic_mean}ï¼ŒICæ ‡å‡†å·®ï¼š{ic_std}ï¼Œicirï¼š{icir}ï¼ŒICèƒœçŽ‡ï¼š{ic_ratio}'

    # è®¡ç®—æ¯æœˆçš„ICçƒ­åŠ›å›¾
    ic_month = ic.resample('ME', on='äº¤æ˜“æ—¥æœŸ').agg({'RankIC': 'mean'})
    ic_month.reset_index(inplace=True)
    # æå–å‡ºå¹´ä»½å’Œæœˆä»½
    ic_month['å¹´ä»½'] = ic_month['äº¤æ˜“æ—¥æœŸ'].dt.year.astype('str')
    ic_month['æœˆä»½'] = ic_month['äº¤æ˜“æ—¥æœŸ'].dt.month
    # å°†å¹´ä»½æœˆä»½è®¾ç½®ä¸ºindexï¼Œåœ¨å°†æœˆä»½unstackä¸ºåˆ—
    ic_month = ic_month.set_index(['å¹´ä»½', 'æœˆä»½'])['RankIC']
    ic_month = ic_month.unstack('æœˆä»½')
    ic_month.columns = ic_month.columns.astype(str)
    # è®¡ç®—å„æœˆå¹³å‡çš„IC
    ic_month.loc['å„æœˆå¹³å‡', :] = ic_month.mean(axis=0)
    # æŒ‰å¹´ä»½å¤§å°æŽ’å
    ic_month = ic_month.sort_index(ascending=False)

    print(f'å› å­ICåˆ†æžå®Œæˆï¼Œè€—æ—¶ï¼š{datetime.datetime.now() - start_time}')
    return ic, ic_info, ic_month


def get_group_net_value(df, cfg):
    print('æ­£åœ¨è¿›è¡Œå› å­åˆ†ç»„åˆ†æž...')
    start_time = datetime.datetime.now()  # è®°å½•å¼€å§‹æ—¶é—´

    # ç”±äºŽä¼šå¯¹åŽŸå§‹çš„æ•°æ®è¿›è¡Œä¿®æ­£ï¼Œæ‰€ä»¥éœ€è¦æŠŠæ•°æ®copyä¸€ä»½
    df['æŒä»“æ”¶ç›Š'] = df['ä¸‹å‘¨æœŸæ¶¨è·Œå¹…'] * cfg.fee_rate
    # æŒ‰ç…§åˆ†ç»„è®¡ç®—èµ„é‡‘æ›²çº¿
    groups = df.groupby(['groups'], observed=False)
    res_list = []
    time_df = pd.DataFrame(sorted(df['äº¤æ˜“æ—¥æœŸ'].unique()), columns=['äº¤æ˜“æ—¥æœŸ'])
    for t, g in groups:
        ret = pd.DataFrame(g.groupby('äº¤æ˜“æ—¥æœŸ')['æŒä»“æ”¶ç›Š'].mean()).reset_index()
        ret['å‡€å€¼'] = (ret['æŒä»“æ”¶ç›Š'] + 1).cumprod()
        ret = pd.merge(ret, time_df, on='äº¤æ˜“æ—¥æœŸ', how='right')
        ret['å‡€å€¼'] = ret['å‡€å€¼'].ffill()
        ret['groups'] = t[0]
        res_list.append(ret[['äº¤æ˜“æ—¥æœŸ', 'å‡€å€¼', 'groups']])
    res_df = pd.concat(res_list, ignore_index=True)
    res_df = pd.DataFrame(res_df.groupby(['äº¤æ˜“æ—¥æœŸ', 'groups'])['å‡€å€¼'].mean()).reset_index()
    group_nv = res_df.pivot(values='å‡€å€¼', index='äº¤æ˜“æ—¥æœŸ', columns='groups')
    group_nv.reset_index(inplace=True)

    for i in range(1, cfg.bins + 1):
        group_nv.rename(columns={i: f'ç¬¬{i}ç»„'}, inplace=True)

    # è®¡ç®—å¤šç©ºå‡€å€¼èµ°åŠ¿
    # èŽ·å–ç¬¬ä¸€ç»„çš„æ¶¨è·Œå¹…æ•°æ®
    first_group_ret = group_nv['ç¬¬1ç»„'].pct_change()
    first_group_ret.fillna(value=group_nv['ç¬¬1ç»„'].iloc[0] - 1, inplace=True)
    # èŽ·å–æœ€åŽä¸€ç»„çš„æ¶¨è·Œå¹…æ•°æ®
    last_group_ret = group_nv[f'ç¬¬{cfg.bins}ç»„'].pct_change()
    last_group_ret.fillna(value=group_nv[f'ç¬¬{cfg.bins}ç»„'].iloc[0] - 1, inplace=True)
    # åˆ¤æ–­åˆ°åº•æ˜¯å¤šç¬¬ä¸€ç»„ç©ºæœ€åŽä¸€ç»„ï¼Œè¿˜æ˜¯å¤šæœ€åŽä¸€ç»„ç©ºç¬¬ä¸€ç»„
    if group_nv['ç¬¬1ç»„'].iloc[-1] > group_nv[f'ç¬¬{cfg.bins}ç»„'].iloc[-1]:
        ls_ret = (first_group_ret - last_group_ret) / 2
    else:
        ls_ret = (last_group_ret - first_group_ret) / 2
    # è®¡ç®—å¤šç©ºå‡€å€¼æ›²çº¿
    group_nv['å¤šç©ºå‡€å€¼'] = (ls_ret + 1).cumprod()

    # è®¡ç®—ç»˜åˆ¶åˆ†ç®±æ‰€éœ€è¦çš„æ•°æ®
    group_value = group_nv[-1:].T[1:].reset_index()
    group_value.columns = ['åˆ†ç»„', 'å‡€å€¼']

    # è®¡ç®—æŒä»“èµ°åŠ¿å›¾
    df['å‘¨æœŸæ•°é‡'] = df['ä¸‹å‘¨æœŸæ¯å¤©æ¶¨è·Œå¹…'].apply(len)
    hold_nums = int(df['å‘¨æœŸæ•°é‡'].mode().iloc[0])
    df['ä¸‹å‘¨æœŸæ¯å¤©æ¶¨è·Œå¹…'] = df['ä¸‹å‘¨æœŸæ¯å¤©æ¶¨è·Œå¹…'].apply(
        lambda x: x[: hold_nums] if len(x) > hold_nums else (x + [0] * (hold_nums - len(x))))
    df['ä¸‹å‘¨æœŸæ¯å¤©å‡€å€¼'] = df['ä¸‹å‘¨æœŸæ¯å¤©æ¶¨è·Œå¹…'].apply(lambda x: (np.array(x) + 1).cumprod())
    df['ä¸‹å‘¨æœŸå‡€å€¼'] = df['ä¸‹å‘¨æœŸæ¯å¤©å‡€å€¼'].apply(lambda x: x[-1] * cfg.fee_rate)

    # è®¡ç®—å„åˆ†ç»„åœ¨æŒä»“å†…çš„æ¯å¤©æ”¶ç›Š
    group_hold_value = pd.DataFrame(df.groupby('groups', observed=False)['ä¸‹å‘¨æœŸæ¯å¤©å‡€å€¼'].mean()).T
    # æ‰€æœ‰åˆ†ç»„çš„ç¬¬ä¸€å¤©éƒ½æ˜¯ä»Ž1å¼€å§‹çš„
    for col in group_hold_value.columns:
        group_hold_value[col] = group_hold_value[col].apply(lambda x: [1] + list(x))
    # å°†æœªæ¥æ”¶ç›Šä»Žlistå±•å¼€æˆé€è¡Œçš„æ•°æ®
    group_hold_value = group_hold_value.explode(list(group_hold_value.columns)).reset_index(drop=True).reset_index()
    # é‡å‘½ååˆ—
    group_cols = ['æ—¶é—´'] + [f'ç¬¬{i}ç»„' for i in range(1, cfg.bins + 1)]
    group_hold_value.columns = group_cols

    print(f'å› å­åˆ†ç»„åˆ†æžå®Œæˆï¼Œè€—æ—¶ï¼š{datetime.datetime.now() - start_time}')

    # è¿”å›žæ•°æ®ï¼šåˆ†ç»„èµ„é‡‘æ›²çº¿ã€åˆ†ç»„æŒä»“èµ°åŠ¿
    return group_nv, group_value, group_hold_value


def get_style_corr(df, cfg):
    print('æ­£åœ¨è¿›è¡Œå› å­é£Žæ ¼æš´éœ²åˆ†æž...')
    start_date = datetime.datetime.now()  # è®°å½•å¼€å§‹æ—¶é—´

    # å–å‡ºé£Žæ ¼åˆ—ï¼Œæ ¼å¼ï¼šä»¥ é£Žæ ¼å› å­_ å¼€å¤´
    style_cols = [col for col in df.columns if 'é£Žæ ¼å› å­_' in col]

    # å¦‚æžœdfä¸­æ²¡æœ‰é£Žæ ¼å› å­åˆ—ï¼Œè¿”å›žç©ºdf
    if len(style_cols) == 0:
        return pd.DataFrame()

    # è®¡ç®—å› å­ä¸Žé£Žæ ¼çš„ç›¸å…³ç³»æ•°
    res = df.groupby('äº¤æ˜“æ—¥æœŸ').apply(
        lambda x: x[[cfg.fa_name] + style_cols].corr(method='spearman').iloc[0, 1:].to_frame())
    style_corr = res.reset_index().groupby('level_1')[cfg.fa_name].mean().reset_index()
    # æ•´ç†æ•°æ®
    style_corr = style_corr.rename(columns={'level_1': 'é£Žæ ¼', cfg.fa_name: 'ç›¸å…³ç³»æ•°'})
    style_corr['é£Žæ ¼'] = style_corr['é£Žæ ¼'].map(lambda x: x.split('_')[1])

    print(f'å› å­é£Žæ ¼åˆ†æžå®Œæˆï¼Œè€—æ—¶ï¼š{datetime.datetime.now() - start_date}')

    return style_corr


def get_class_ic_and_pct(df, cfg, is_industry=True):
    print('æ­£åœ¨è¿›è¡Œå› å­è¡Œä¸šåˆ†æž...' if is_industry else 'æ­£åœ¨è¿›è¡Œå› å­å¸‚å€¼åˆ†ç»„åˆ†æž...')
    start_date = datetime.datetime.now()  # è®°å½•å¼€å§‹æ—¶é—´

    # å¦‚æžœæ˜¯è¡Œä¸šåˆ†ç»„
    if is_industry:
        class_col = 'æ–°ç‰ˆç”³ä¸‡ä¸€çº§è¡Œä¸šåç§°'
        class_name = 'è¡Œä¸š'
        import warnings
        warnings.filterwarnings('ignore')
        # æ›¿æ¢è¡Œä¸šåç§°
        df['æ–°ç‰ˆç”³ä¸‡ä¸€çº§è¡Œä¸šåç§°'] = df['æ–°ç‰ˆç”³ä¸‡ä¸€çº§è¡Œä¸šåç§°'].replace(cfg.ind_name_change)
    else:  # æŒ‰ç…§å¸‚å€¼è¿›è¡Œåˆ†ç»„
        class_col = 'å¸‚å€¼åˆ†ç»„'
        class_name = 'å¸‚å€¼åˆ†ç»„'
        # å…ˆå¯¹å¸‚å€¼æ•°æ®è¿›è¡ŒæŽ’åä»¥åŠåˆ†ç»„
        df['å¸‚å€¼åˆ†ç»„'] = df.groupby(['äº¤æ˜“æ—¥æœŸ'])['é£Žæ ¼å› å­_è§„æ¨¡'].transform(
            lambda x: pd.qcut(x, q=cfg.bins, labels=range(1, cfg.bins + 1), duplicates='drop'))

    def get_data(temp):
        """
        è®¡ç®—åˆ†è¡Œä¸šICã€å æ¯”
        :param temp: æ¯ä¸ªè¡Œä¸šçš„æ•°æ®
        :return:
            è¿”å›žICåºåˆ—çš„å‡å€¼ã€ç¬¬ä¸€ç»„å æ¯”ã€æœ€åŽä¸€ç»„å æ¯”
        """
        # è®¡ç®—æ¯ä¸ªè¡Œä¸šçš„ICåºåˆ—
        ic = temp.groupby('äº¤æ˜“æ—¥æœŸ').apply(lambda x: x[cfg.fa_name].corr(x['ä¸‹å‘¨æœŸæ¶¨è·Œå¹…'], method='spearman'))
        # æ•´ç†ICæ•°æ®
        ic = ic.to_frame().reset_index().rename(columns={0: 'RankIC'})

        # è®¡ç®—æ¯ä¸ªè¡Œä¸šçš„ç¬¬ä¸€ç»„çš„å æ¯”å’Œæœ€åŽä¸€ç»„çš„å æ¯”
        part_min_group = temp.groupby('äº¤æ˜“æ—¥æœŸ').apply(lambda x: (x['groups'] == min_group).sum())
        part_max_group = temp.groupby('äº¤æ˜“æ—¥æœŸ').apply(lambda x: (x['groups'] == max_group).sum())
        part_min_group = part_min_group / all_min_group
        part_max_group = part_max_group / all_max_group
        # æ•´ç†å æ¯”æ•°æ®
        part_min_group = part_min_group.to_frame().reset_index().rename(
            columns={0: f'å› å­ç¬¬ä¸€ç»„é€‰è‚¡åœ¨å„{class_name}çš„å æ¯”'})
        part_max_group = part_max_group.to_frame().reset_index().rename(
            columns={0: f'å› å­æœ€åŽä¸€ç»„é€‰è‚¡åœ¨å„{class_name}çš„å æ¯”'})

        # å°†å„ä¸ªæ•°æ®åˆå¹¶ä¸€ä¸‹
        data = pd.merge(ic, part_min_group, on='äº¤æ˜“æ—¥æœŸ', how='inner')
        data = pd.merge(data, part_max_group, on='äº¤æ˜“æ—¥æœŸ', how='inner')
        data.set_index('äº¤æ˜“æ—¥æœŸ', inplace=True)  # è®¾ç½®ä¸‹ç´¢å¼•

        return data

    # èŽ·å–ä»¥å› å­åˆ†ç»„ç¬¬ä¸€ç»„å’Œæœ€åŽä¸€ç»„çš„æ•°é‡
    min_group, max_group = df['groups'].min(), df['groups'].max()
    all_min_group = df.groupby('äº¤æ˜“æ—¥æœŸ').apply(lambda x: (x['groups'] == min_group).sum())
    all_max_group = df.groupby('äº¤æ˜“æ—¥æœŸ').apply(lambda x: (x['groups'] == max_group).sum())
    # ä»¥è¡Œä¸šåˆ†ç»„è®¡ç®—ICåŠå æ¯”ï¼Œå¹¶å¤„ç†æ•°æ®
    class_data = df.groupby(class_col, observed=False).apply(get_data).reset_index()

    # å¯¹æ¯ä¸ªè¡Œä¸šæ±‚ICå‡å€¼ã€è¡Œä¸šå æ¯”ç¬¬ä¸€ç»„å‡å€¼ã€è¡Œä¸šå æ¯”æœ€åŽä¸€ç»„å‡å€¼
    class_data = class_data.groupby(class_col, observed=False).apply(
        lambda x: [x['RankIC'].mean(), x[f'å› å­ç¬¬ä¸€ç»„é€‰è‚¡åœ¨å„{class_name}çš„å æ¯”'].mean(),
                   x[f'å› å­æœ€åŽä¸€ç»„é€‰è‚¡åœ¨å„{class_name}çš„å æ¯”'].mean()])
    class_data = class_data.to_frame().reset_index()  # æ•´ç†æ•°æ®
    # å–å‡ºICæ•°æ®ã€è¡Œä¸šå æ¯”_ç¬¬ä¸€ç»„æ•°æ®ã€è¡Œä¸šå æ¯”_æœ€åŽä¸€ç»„æ•°æ®
    class_data['RankIC'] = class_data[0].map(lambda x: x[0])
    class_data[f'å› å­ç¬¬ä¸€ç»„é€‰è‚¡åœ¨å„{class_name}çš„å æ¯”'] = class_data[0].map(lambda x: x[1])
    class_data[f'å› å­æœ€åŽä¸€ç»„é€‰è‚¡åœ¨å„{class_name}çš„å æ¯”'] = class_data[0].map(lambda x: x[2])
    # å¤„ç†æ•°æ®
    class_data.drop(0, axis=1, inplace=True)
    # ä»¥ICæŽ’åº
    class_data.sort_values('RankIC', ascending=False, inplace=True)

    print(f'å› å­{class_col}åˆ†æžå®Œæˆï¼Œè€—æ—¶ï¼š{datetime.datetime.now() - start_date}')
    return class_data


def get_factor_score(ic, group_value):
    max_net = max(group_value['å‡€å€¼'].iloc[0], group_value['å‡€å€¼'].iloc[-2])
    icir = ic['RankIC'].mean() / ic['RankIC'].std()
    rank_corr = np.corrcoef(list(group_value['å‡€å€¼'][:-1]), list(range(1, len(group_value))))[0, 1]
    score = max_net * abs(icir) * abs(rank_corr)
    return score


# endregion

# region åŒå› å­åˆ†æžè¦ç”¨åˆ°çš„å‡½æ•°
def double_factor_grouping(df, cfg):
    print(f'æ­£åœ¨å¯¹åŒå› å­ {cfg.main} å’Œ {cfg.sub} åˆ†ç»„...')
    start_date = datetime.datetime.now()  # è®°å½•å¼€å§‹æ—¶é—´

    # æ ¹æ®ä¸»å› å­è®¡ç®—ä¸»å› å­çš„æŽ’åmethod='min'ä¸Žé£Žæ ¼å› å­ä¿æŒç›¸åŒå–æ³•
    df['æŽ’å_ä¸»å› å­'] = df.groupby(['äº¤æ˜“æ—¥æœŸ'], observed=False)[cfg.main].rank(ascending=True, method='first')
    # æ ¹æ®æ¬¡å› å­è®¡ç®—æ¬¡å› å­çš„æŽ’å
    df['æŽ’å_æ¬¡å› å­'] = df.groupby(['äº¤æ˜“æ—¥æœŸ'], observed=False)[cfg.sub].rank(ascending=True, method='first')
    # æ ¹æ®ä¸»å› å­çš„æŽ’åè¿›è¡Œåˆ†ç»„
    df['groups_ä¸»å› å­'] = df.groupby(['äº¤æ˜“æ—¥æœŸ'], observed=False)['æŽ’å_ä¸»å› å­'].transform(
        lambda x: pd.qcut(x, q=cfg.bins, labels=range(1, cfg.bins + 1), duplicates='drop'))
    # æ ¹æ®æ¬¡å› å­çš„æŽ’åè¿›è¡Œåˆ†ç»„
    df['groups_æ¬¡å› å­'] = df.groupby(['äº¤æ˜“æ—¥æœŸ'], observed=False)['æŽ’å_æ¬¡å› å­'].transform(
        lambda x: pd.qcut(x, q=cfg.bins, labels=range(1, cfg.bins + 1), duplicates='drop'))
    # åœ¨ä¸»å› å­åˆ†ç»„åŸºç¡€ä¸Šï¼Œå†æ ¹æ®æ¬¡å› å­çš„æŽ’åè¿›è¡Œåˆ†ç»„
    df['groups_ä¸»å› å­åˆ†ç®±_æ¬¡å› å­'] = df.groupby(['äº¤æ˜“æ—¥æœŸ', 'groups_ä¸»å› å­'], observed=False)['æŽ’å_æ¬¡å› å­'].transform(
        lambda x: pd.qcut(x, q=cfg.bins, labels=range(1, cfg.bins + 1), duplicates='drop'))
    # åœ¨æ¬¡å› å­åˆ†ç»„åŸºç¡€ä¸Šï¼Œå†æ ¹æ®ä¸»å› å­çš„æŽ’åè¿›è¡Œåˆ†ç»„
    df['groups_æ¬¡å› å­åˆ†ç®±_ä¸»å› å­'] = df.groupby(['äº¤æ˜“æ—¥æœŸ', 'groups_æ¬¡å› å­'], observed=False)['æŽ’å_ä¸»å› å­'].transform(
        lambda x: pd.qcut(x, q=cfg.bins, labels=range(1, cfg.bins + 1), duplicates='drop'))

    # è¿™é‡Œä¸éœ€è¦åˆ¤æ–­æŸä¸ªå‘¨æœŸçš„è‚¡ç¥¨æ•°é‡å¤§äºŽbinsï¼Œå› ä¸ºä¹‹å‰åœ¨å¤„ç†limitæ—¶å·²ç»å¤„ç†è¿‡è¿™ä¸ªé—®é¢˜
    print(f'åŒå› å­ {cfg.main} å’Œ {cfg.sub} åˆ†ç»„å®Œæˆï¼Œè€—æ—¶ï¼š{datetime.datetime.now() - start_date}')
    return df


def get_group_nv_double(df, cfg):
    """
    é’ˆå¯¹åŒå› å­åˆ†ç»„æ•°æ®è¿›è¡Œåˆ†æžï¼Œç»™å‡ºåŒå› å­åˆ†ç»„çš„ç»„åˆå¹³å‡æ”¶ç›Šã€è¿‡æ»¤å¹³å‡æ”¶ç›Šæ•°æ®
    :param df: è¾“å…¥çš„æ•°æ®
    :param cfg: é…ç½®
    :return:
        è¿”å›žåŒå› å­ç»„åˆåˆ†ç»„å¹³å‡æ”¶ç›Šã€åŒå› å­ç»„åˆåˆ†ç»„å¹³å‡å æ¯”ã€åŒå› å­è¿‡æ»¤åˆ†ç»„å¹³å‡æ”¶ç›Šæ•°æ®
    """

    print('è®¡ç®—åŒå› å­å¹³å‡æ”¶ç›Š...')
    start_date = datetime.datetime.now()  # è®°å½•å¼€å§‹æ—¶é—´

    # ç”±äºŽä¼šå¯¹åŽŸå§‹çš„æ•°æ®è¿›è¡Œä¿®æ­£ï¼Œæ‰€ä»¥éœ€è¦æŠŠæ•°æ®copyä¸€ä»½
    temp = df.copy()

    # è®¡ç®—ä¸‹å‘¨æœŸæ¯å¤©çš„å‡€å€¼ï¼Œå¹¶æ‰£é™¤æ‰‹ç»­è´¹å¾—åˆ°ä¸‹å‘¨æœŸçš„å®žé™…å‡€å€¼
    temp['ä¸‹å‘¨æœŸæ¯å¤©å‡€å€¼'] = temp['ä¸‹å‘¨æœŸæ¯å¤©æ¶¨è·Œå¹…'].apply(lambda x: (np.array(x) + 1).cumprod())
    temp['ä¸‹å‘¨æœŸå¹³å‡æ”¶ç›Š'] = temp['ä¸‹å‘¨æœŸæ¯å¤©å‡€å€¼'].apply(lambda x: np.power((x[-1] * cfg.fee_rate), 1 / len(x)) - 1)

    # è®¡ç®—åŒå› å­ç»„åˆåˆ†ç»„åœ¨æŒä»“å†…çš„å¹³å‡æ”¶ç›Š
    mix_nv = temp.groupby(['groups_ä¸»å› å­', 'groups_æ¬¡å› å­'], observed=False)['ä¸‹å‘¨æœŸå¹³å‡æ”¶ç›Š'].mean().reset_index()

    # è®¡ç®—åŒå› å­ç»„åˆåˆ†ç»„åœ¨æŒä»“å†…çš„è‚¡ç¥¨å æ¯”
    mix_prop = temp.groupby(['äº¤æ˜“æ—¥æœŸ', 'groups_ä¸»å› å­', 'groups_æ¬¡å› å­'], observed=False).agg(
        {'è‚¡ç¥¨åç§°': 'count', 'å½“å‘¨æœŸè‚¡ç¥¨æ•°': 'last'}).reset_index()
    mix_prop['å½“å‘¨æœŸå¹³å‡å æ¯”'] = mix_prop['è‚¡ç¥¨åç§°'] / mix_prop['å½“å‘¨æœŸè‚¡ç¥¨æ•°']
    mix_prop['å½“å‘¨æœŸå¹³å‡å æ¯”'] = mix_prop['å½“å‘¨æœŸå¹³å‡å æ¯”'].fillna(0)
    mix_prop = mix_prop.groupby(['groups_ä¸»å› å­', 'groups_æ¬¡å› å­'], observed=False)[
        'å½“å‘¨æœŸå¹³å‡å æ¯”'].mean().reset_index()

    # è®¡ç®—åŒå› å­è¿‡æ»¤åˆ†ç»„åœ¨æŒä»“å†…çš„å¹³å‡æ”¶ç›Š ä¸»->æ¬¡
    filter_nv_ms = temp.groupby(['groups_ä¸»å› å­', 'groups_ä¸»å› å­åˆ†ç®±_æ¬¡å› å­'], observed=False)[
        'ä¸‹å‘¨æœŸå¹³å‡æ”¶ç›Š'].mean().reset_index()

    # è®¡ç®—åŒå› å­è¿‡æ»¤åˆ†ç»„åœ¨æŒä»“å†…çš„å¹³å‡æ”¶ç›Š æ¬¡->ä¸»
    filter_nv_sm = temp.groupby(['groups_æ¬¡å› å­', 'groups_æ¬¡å› å­åˆ†ç®±_ä¸»å› å­'], observed=False)[
        'ä¸‹å‘¨æœŸå¹³å‡æ”¶ç›Š'].mean().reset_index()

    # ä¸‹å‘¨æœŸå¹³å‡æ”¶ç›Šè½¬æ¢å•ä½åƒåˆ†ä¹‹,å½“å‘¨æœŸå¹³å‡å æ¯”è½¬æ¢å•ä½ç™¾åˆ†ä¹‹
    mix_nv['ä¸‹å‘¨æœŸå¹³å‡æ”¶ç›Š'] = mix_nv['ä¸‹å‘¨æœŸå¹³å‡æ”¶ç›Š'].apply(lambda x: x * 1000)
    mix_prop['å½“å‘¨æœŸå¹³å‡å æ¯”'] = mix_prop['å½“å‘¨æœŸå¹³å‡å æ¯”'].apply(lambda x: x * 100)
    filter_nv_ms['ä¸‹å‘¨æœŸå¹³å‡æ”¶ç›Š'] = filter_nv_ms['ä¸‹å‘¨æœŸå¹³å‡æ”¶ç›Š'].apply(lambda x: x * 1000)
    filter_nv_sm['ä¸‹å‘¨æœŸå¹³å‡æ”¶ç›Š'] = filter_nv_sm['ä¸‹å‘¨æœŸå¹³å‡æ”¶ç›Š'].apply(lambda x: x * 1000)

    # å°†groups_æ¬¡å› å­ã€groups_ä¸»å› å­è®¾ç½®ä¸ºindexï¼Œåœ¨å°†groups_ä¸»å› å­ä¸ºåˆ—
    mix_nv['groups_ä¸»å› å­'] = mix_nv['groups_ä¸»å› å­'].apply(lambda x: 'ä¸»å› å­' + str(x))
    mix_nv['groups_æ¬¡å› å­'] = mix_nv['groups_æ¬¡å› å­'].apply(lambda x: 'æ¬¡å› å­' + str(x))
    mix_nv = mix_nv.set_index(['groups_æ¬¡å› å­', 'groups_ä¸»å› å­'])['ä¸‹å‘¨æœŸå¹³å‡æ”¶ç›Š']
    mix_nv = mix_nv.unstack('groups_ä¸»å› å­')
    # æ·»åŠ å¹³å‡æ”¶ç›Š
    mix_nv.loc['ä¸»å› å­å¹³å‡æ”¶ç›Š'] = mix_nv.mean()
    mix_nv['æ¬¡å› å­å¹³å‡æ”¶ç›Š'] = mix_nv.mean(axis=1)

    mix_prop['groups_ä¸»å› å­'] = mix_prop['groups_ä¸»å› å­'].apply(lambda x: 'ä¸»å› å­' + str(x))
    mix_prop['groups_æ¬¡å› å­'] = mix_prop['groups_æ¬¡å› å­'].apply(lambda x: 'æ¬¡å› å­' + str(x))
    mix_prop = mix_prop.set_index(['groups_æ¬¡å› å­', 'groups_ä¸»å› å­'])['å½“å‘¨æœŸå¹³å‡å æ¯”']
    mix_prop = mix_prop.unstack('groups_ä¸»å› å­')

    # è®¡ç®—åŒå› å­è¿‡æ»¤ç»„åˆä¸»å› å­åˆ†ç®±å¹³å‡æ”¶ç›Šï¼Œä¸»å› å­åˆ†ç»„çš„åŸºç¡€ä¸Šï¼Œæ¬¡å› å­å†åˆ†ç»„
    filter_nv_main_mean = filter_nv_ms.groupby(['groups_ä¸»å› å­'], observed=False).agg(
        {'groups_ä¸»å› å­åˆ†ç®±_æ¬¡å› å­': 'first', 'ä¸‹å‘¨æœŸå¹³å‡æ”¶ç›Š': 'mean'}).reset_index()
    filter_nv_main_mean['groups_ä¸»å› å­åˆ†ç®±_æ¬¡å› å­'] = 0
    filter_nv_ms = pd.concat([filter_nv_ms, filter_nv_main_mean], ignore_index=True)
    filter_nv_ms['groups_ä¸»å› å­'] = filter_nv_ms['groups_ä¸»å› å­'].astype(int)
    filter_nv_ms['groups_ä¸»å› å­åˆ†ç®±_æ¬¡å› å­'] = filter_nv_ms['groups_ä¸»å› å­åˆ†ç®±_æ¬¡å› å­'].astype(int)
    filter_nv_ms.sort_values(by=['groups_ä¸»å› å­', 'groups_ä¸»å› å­åˆ†ç®±_æ¬¡å› å­'], inplace=True, ignore_index=True)

    filter_nv_ms = filter_nv_ms.set_index(['groups_ä¸»å› å­åˆ†ç®±_æ¬¡å› å­', 'groups_ä¸»å› å­'])['ä¸‹å‘¨æœŸå¹³å‡æ”¶ç›Š']
    filter_nv_ms = filter_nv_ms.unstack('groups_ä¸»å› å­')
    # æ ¹æ®binsçš„æ•°é‡æ¥é‡å‘½å
    rename_dict = {i: f'ä¸»å› å­{i}' for i in range(1, cfg.bins + 1)}
    filter_nv_ms.rename(columns=rename_dict, inplace=True)
    rename_dict = {i: f'æ¬¡å› å­{i}' for i in range(1, cfg.bins + 1)}
    rename_dict[0] = 'ä¸»å› å­å¹³å‡æ”¶ç›Š'
    filter_nv_ms.rename(index=rename_dict, inplace=True)
    filter_nv_ms.loc['ä¸»å› å­å¹³å‡æ”¶ç›Š'] = filter_nv_ms.mean()
    filter_nv_ms['æ¬¡å› å­å¹³å‡æ”¶ç›Š'] = filter_nv_ms.mean(axis=1)

    # è®¡ç®—åŒå› å­è¿‡æ»¤ç»„åˆä¸»å› å­åˆ†ç®±å¹³å‡æ”¶ç›Šï¼Œæ¬¡å› å­åˆ†ç»„çš„åŸºç¡€ä¸Šï¼Œä¸»å› å­å†åˆ†ç»„
    filter_nv_sub_mean = filter_nv_sm.groupby(['groups_æ¬¡å› å­'], observed=False).agg(
        {'groups_æ¬¡å› å­åˆ†ç®±_ä¸»å› å­': 'first', 'ä¸‹å‘¨æœŸå¹³å‡æ”¶ç›Š': 'mean'}).reset_index()
    filter_nv_sub_mean['groups_æ¬¡å› å­åˆ†ç®±_ä¸»å› å­'] = 0
    filter_nv_sm = pd.concat([filter_nv_sm, filter_nv_sub_mean], ignore_index=True)
    filter_nv_sm['groups_æ¬¡å› å­'] = filter_nv_sm['groups_æ¬¡å› å­'].astype(int)
    filter_nv_sm['groups_æ¬¡å› å­åˆ†ç®±_ä¸»å› å­'] = filter_nv_sm['groups_æ¬¡å› å­åˆ†ç®±_ä¸»å› å­'].astype(int)
    filter_nv_sm.sort_values(by=['groups_æ¬¡å› å­', 'groups_æ¬¡å› å­åˆ†ç®±_ä¸»å› å­'], inplace=True, ignore_index=True)
    filter_nv_sm = filter_nv_sm.set_index(['groups_æ¬¡å› å­åˆ†ç®±_ä¸»å› å­', 'groups_æ¬¡å› å­'])['ä¸‹å‘¨æœŸå¹³å‡æ”¶ç›Š']
    filter_nv_sm = filter_nv_sm.unstack('groups_æ¬¡å› å­')
    # æ ¹æ®binsçš„æ•°é‡æ¥é‡å‘½å
    rename_dict = {i: f'æ¬¡å› å­{i}' for i in range(1, cfg.bins + 1)}
    filter_nv_sm.rename(columns=rename_dict, inplace=True)
    rename_dict = {i: f'ä¸»å› å­{i}' for i in range(1, cfg.bins + 1)}
    rename_dict[0] = 'æ¬¡å› å­å¹³å‡æ”¶ç›Š'
    filter_nv_sm.rename(index=rename_dict, inplace=True)
    filter_nv_sm.loc['æ¬¡å› å­å¹³å‡æ”¶ç›Š'] = filter_nv_sm.mean()
    filter_nv_sm['ä¸»å› å­å¹³å‡æ”¶ç›Š'] = filter_nv_sm.mean(axis=1)

    print(f'è®¡ç®—åŒå› å­å¹³å‡æ”¶ç›Šå®Œæˆï¼Œè€—æ—¶ï¼š{datetime.datetime.now() - start_date}')
    return mix_nv, mix_prop, filter_nv_ms, filter_nv_sm


def get_style_corr_double(df, cfg):
    print('æ­£åœ¨è¿›è¡Œå› å­é£Žæ ¼æš´éœ²åˆ†æž...')
    start_date = datetime.datetime.now()  # è®°å½•å¼€å§‹æ—¶é—´

    # ç”±äºŽä¼šå¯¹åŽŸå§‹çš„æ•°æ®è¿›è¡Œä¿®æ­£ï¼Œæ‰€ä»¥éœ€è¦æŠŠæ•°æ®copyä¸€ä»½
    temp = df.copy()

    temp['æŽ’å_ä¸»å› å­'] = temp.groupby(['äº¤æ˜“æ—¥æœŸ'])[cfg.main].rank(ascending=True, method='first')
    # æ ¹æ®æ¬¡å› å­è®¡ç®—æ¬¡å› å­çš„æŽ’å
    temp['æŽ’å_æ¬¡å› å­'] = temp.groupby(['äº¤æ˜“æ—¥æœŸ'])[cfg.sub].rank(ascending=True, method='first')

    # è®¡ç®—å› å­ICå€¼
    main_factor_ic = df.groupby('äº¤æ˜“æ—¥æœŸ').apply(
        lambda x: x[cfg.main].corr(x['ä¸‹å‘¨æœŸæ¶¨è·Œå¹…'], method='spearman')).to_frame()
    main_factor_ic = main_factor_ic.rename(columns={0: 'RankIC'}).reset_index()
    main_factor_ic_mean = main_factor_ic['RankIC'].mean()
    sub_factor_ic = df.groupby('äº¤æ˜“æ—¥æœŸ').apply(
        lambda x: x[cfg.sub].corr(x['ä¸‹å‘¨æœŸæ¶¨è·Œå¹…'], method='spearman')).to_frame()
    sub_factor_ic = sub_factor_ic.rename(columns={0: 'RankIC'}).reset_index()
    sub_factor_ic_mean = sub_factor_ic['RankIC'].mean()
    double_factor_ic_flag = 1 if main_factor_ic_mean * sub_factor_ic_mean >= 0 else -1

    # è®¡ç®—åŒå› å­ç­‰æƒ
    temp['é£Žæ ¼å› å­_åŒå› å­'] = temp['æŽ’å_ä¸»å› å­'] + temp['æŽ’å_æ¬¡å› å­'] * double_factor_ic_flag

    # å–å‡ºé£Žæ ¼åˆ—ï¼Œæ ¼å¼ï¼šä»¥ é£Žæ ¼å› å­_ å¼€å¤´
    factor_style_cols = [col for col in temp.columns if 'é£Žæ ¼å› å­_' in col]

    def func(x, factor, style):
        if len(x) > 100:
            res = x[[factor] + style].corr(method='spearman').iloc[0, 1:].to_frame()
        else:
            res = pd.Series()
        return res

    temp.dropna(subset=['æŽ’å_æ¬¡å› å­', 'æŽ’å_æ¬¡å› å­', 'é£Žæ ¼å› å­_åŒå› å­'] + factor_style_cols, inplace=True)
    main_res = temp.groupby('äº¤æ˜“æ—¥æœŸ').apply(lambda x: func(x, 'æŽ’å_ä¸»å› å­', factor_style_cols))
    main_factor_style_corr = main_res.reset_index().groupby('level_1')['æŽ’å_ä¸»å› å­'].mean().reset_index()

    sub_res = temp.groupby('äº¤æ˜“æ—¥æœŸ').apply(lambda x: func(x, 'æŽ’å_æ¬¡å› å­', factor_style_cols))
    sub_factor_style_corr = sub_res.reset_index().groupby('level_1')['æŽ’å_æ¬¡å› å­'].mean().reset_index()

    double_res = temp.groupby('äº¤æ˜“æ—¥æœŸ').apply(lambda x: func(x, 'é£Žæ ¼å› å­_åŒå› å­', factor_style_cols))
    double_factor_style_corr = double_res.reset_index().groupby('level_1')['é£Žæ ¼å› å­_åŒå› å­'].mean().reset_index()

    # é£Žæ ¼å› å­_åŒå› å­ è¿™é‡Œæ˜¯ä¸»æ¬¡å› å­çš„ç›¸å…³ç³»æ•°
    max_inx = double_factor_style_corr.idxmax()
    double_factor_style_corr.loc[max_inx, 'é£Žæ ¼å› å­_åŒå› å­'] = \
        temp[[cfg.main, cfg.sub]].corr(method='spearman').iloc[0, 1]

    # æ•´ç†æ•°æ®
    main_factor_style_corr = main_factor_style_corr.rename(
        columns={'level_1': 'é£Žæ ¼', 'æŽ’å_ä¸»å› å­': 'ç›¸å…³ç³»æ•°_ä¸»å› å­'})
    sub_factor_style_corr = sub_factor_style_corr.rename(columns={'level_1': 'é£Žæ ¼', 'æŽ’å_æ¬¡å› å­': 'ç›¸å…³ç³»æ•°_æ¬¡å› å­'})
    double_factor_style_corr = double_factor_style_corr.rename(
        columns={'level_1': 'é£Žæ ¼', 'é£Žæ ¼å› å­_åŒå› å­': 'ç›¸å…³ç³»æ•°_åŒå› å­'})

    # åˆå¹¶æ•°æ®å¹¶è®¾ç½®offset
    style_corr = pd.merge(main_factor_style_corr, sub_factor_style_corr, how='left', on='é£Žæ ¼')
    style_corr = pd.merge(style_corr, double_factor_style_corr, how='left', on='é£Žæ ¼')
    style_corr['é£Žæ ¼'] = style_corr['é£Žæ ¼'].apply(lambda x: x.split('_')[1])

    # èŽ·å–åŒå› å­ç›¸å…³ç³»æ•°
    main_sub_corr = float_num_process(style_corr[style_corr['é£Žæ ¼'] == 'åŒå› å­']['ç›¸å…³ç³»æ•°_åŒå› å­'].iloc[-1])
    main_comp_corr = float_num_process(style_corr[style_corr['é£Žæ ¼'] == 'åŒå› å­']['ç›¸å…³ç³»æ•°_ä¸»å› å­'].iloc[-1])
    sub_comp_corr = float_num_process(style_corr[style_corr['é£Žæ ¼'] == 'åŒå› å­']['ç›¸å…³ç³»æ•°_æ¬¡å› å­'].iloc[-1])

    corr_txt = f'corr(ä¸»ï¼Œæ¬¡)ï¼š{main_sub_corr}    corr(ä¸»ï¼Œå¤)ï¼š{main_comp_corr}    corr(æ¬¡ï¼Œå¤)ï¼š{sub_comp_corr}'
    # åˆ é™¤åŒå› å­ç›¸å…³ç³»æ•°
    style_corr = style_corr[style_corr['é£Žæ ¼'] != 'åŒå› å­']
    print(f'å› å­é£Žæ ¼åˆ†æžå®Œæˆï¼Œè€—æ—¶ï¼š{datetime.datetime.now() - start_date}')
    return style_corr, corr_txt


# endregion

def read_period_and_offset_file(file_path):
    """
    è½½å…¥å‘¨æœŸoffsetæ–‡ä»¶
    """
    if os.path.exists(file_path):
        df = pd.read_csv(file_path, encoding='gbk', parse_dates=['äº¤æ˜“æ—¥æœŸ'], skiprows=1)
        return df
    else:
        print(f'æ–‡ä»¶{file_path}ä¸å­˜åœ¨ï¼Œè¯·èŽ·å–period_offset.csvæ–‡ä»¶åŽå†è¯•')
        raise FileNotFoundError('æ–‡ä»¶ä¸å­˜åœ¨')


def import_index_data(path, date_range=(None, None), max_param=0):
    """
    å¯¼å…¥æŒ‡æ•°æ•°æ®å¹¶è¿›è¡Œé¢„å¤„ç†

    å‚æ•°:
    path (str): æŒ‡æ•°æ•°æ®æ–‡ä»¶çš„è·¯å¾„
    date_range (list, optional): å›žæµ‹çš„æ—¶é—´èŒƒå›´ï¼Œæ ¼å¼ä¸º [å¼€å§‹æ—¥æœŸ, ç»“æŸæ—¥æœŸ]ï¼Œé»˜è®¤ä¸º [None, None]
    max_param (int, optional): å› å­çš„æœ€å¤§å‘¨æœŸæ•°ï¼Œç”¨äºŽæŽ§åˆ¶å¼€å§‹æ—¥æœŸï¼Œç¡®ä¿rollingç±»å› å­ï¼Œå‰ç½®æ•°æ®ä¸æ˜¯NaNï¼Œé»˜è®¤ä¸º 0

    è¿”å›ž:
    DataFrame: å¤„ç†åŽçš„æŒ‡æ•°æ•°æ®ï¼ŒåŒ…å«äº¤æ˜“æ—¥æœŸå’ŒæŒ‡æ•°æ¶¨è·Œå¹…
    """
    # å¯¼å…¥æŒ‡æ•°æ•°æ®
    df_index = pd.read_csv(path, parse_dates=['candle_end_time'], encoding='gbk')

    # è®¡ç®—æ¶¨è·Œå¹…
    df_index['æŒ‡æ•°æ¶¨è·Œå¹…'] = df_index['close'].pct_change()
    # ç¬¬ä¸€å¤©çš„æŒ‡æ•°æ¶¨è·Œå¹…æ˜¯å¼€ç›˜ä¹°å…¥çš„æ¶¨è·Œå¹…
    df_index['æŒ‡æ•°æ¶¨è·Œå¹…'] = df_index['æŒ‡æ•°æ¶¨è·Œå¹…'].fillna(value=df_index['close'] / df_index['open'] - 1)

    # åŽ»é™¤æ¶¨è·Œå¹…ä¸ºç©ºçš„è¡Œ
    df_index.dropna(subset=['æŒ‡æ•°æ¶¨è·Œå¹…'], inplace=True)

    # é‡å‘½ååˆ—
    df_index.rename(columns={'candle_end_time': 'äº¤æ˜“æ—¥æœŸ'}, inplace=True)

    # æ ¹æ®æ—¥æœŸèŒƒå›´è¿‡æ»¤æ•°æ®
    if date_range[0]:
        if max_param == 0:
            df_index = df_index[df_index['äº¤æ˜“æ—¥æœŸ'] >= pd.to_datetime(date_range[0])]
            # print(f'ðŸ’¡ å›žæµ‹å¼€å§‹æ—¶é—´ï¼š{df_index["äº¤æ˜“æ—¥æœŸ"].iloc[0].strftime("%Y-%m-%d")}')
        # å½“æä¾›äº†å‘¨æœŸæ•°ä¹‹åŽ
        else:
            # è®¡ç®—æ–°çš„å¼€å§‹æ—¥æœŸ
            start_index = df_index[df_index['äº¤æ˜“æ—¥æœŸ'] >= pd.to_datetime(date_range[0])].index[0]
            start_date = df_index['äº¤æ˜“æ—¥æœŸ'][start_index].strftime("%Y-%m-%d")

            # ç§»åŠ¨å‘¨æœŸï¼ŒèŽ·å–å¯ä»¥è®©å› å­æ•°å€¼ä¸ä¸ºNançš„å¼€å§‹æ—¥æœŸ
            shifted_date = df_index['äº¤æ˜“æ—¥æœŸ'].shift(max_param)
            shifted_date.bfill(inplace=True)  # å‰ç½®æ•°æ®ä¸æ˜¯NaN

            # è¿‡æ»¤å‰ç½®æ•°æ®
            df_index = df_index[df_index['äº¤æ˜“æ—¥æœŸ'] >= shifted_date[start_index]]
            new_start_date = df_index['äº¤æ˜“æ—¥æœŸ'].iloc[0].strftime("%Y-%m-%d")
            print(f'ðŸ’¡ å›žæµ‹å¼€å§‹æ—¶é—´ï¼š{start_date}ï¼Œç§»åŠ¨{max_param}ä¸ªå‘¨æœŸï¼Œæœ€æ–°äº¤æ˜“æ—¥ï¼š{new_start_date}')
    if date_range[1]:
        df_index = df_index[df_index['äº¤æ˜“æ—¥æœŸ'] <= pd.to_datetime(date_range[1])]
        # print(f'å›žæµ‹ç»“æŸæ—¶é—´ï¼š{df_index["äº¤æ˜“æ—¥æœŸ"].iloc[-1].strftime("%Y-%m-%d")}')

    # æŒ‰æ—¶é—´æŽ’åºå¹¶é‡ç½®ç´¢å¼•
    df_index.sort_values(by=['äº¤æ˜“æ—¥æœŸ'], inplace=True)
    df_index.reset_index(inplace=True, drop=True)

    return df_index
